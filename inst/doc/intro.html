<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="sa21204151" />


<title>intro</title>

<script src="data:application/javascript;base64,Ly8gUGFuZG9jIDIuOSBhZGRzIGF0dHJpYnV0ZXMgb24gYm90aCBoZWFkZXIgYW5kIGRpdi4gV2UgcmVtb3ZlIHRoZSBmb3JtZXIgKHRvCi8vIGJlIGNvbXBhdGlibGUgd2l0aCB0aGUgYmVoYXZpb3Igb2YgUGFuZG9jIDwgMi44KS4KZG9jdW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignRE9NQ29udGVudExvYWRlZCcsIGZ1bmN0aW9uKGUpIHsKICB2YXIgaHMgPSBkb2N1bWVudC5xdWVyeVNlbGVjdG9yQWxsKCJkaXYuc2VjdGlvbltjbGFzcyo9J2xldmVsJ10gPiA6Zmlyc3QtY2hpbGQiKTsKICB2YXIgaSwgaCwgYTsKICBmb3IgKGkgPSAwOyBpIDwgaHMubGVuZ3RoOyBpKyspIHsKICAgIGggPSBoc1tpXTsKICAgIGlmICghL15oWzEtNl0kL2kudGVzdChoLnRhZ05hbWUpKSBjb250aW51ZTsgIC8vIGl0IHNob3VsZCBiZSBhIGhlYWRlciBoMS1oNgogICAgYSA9IGguYXR0cmlidXRlczsKICAgIHdoaWxlIChhLmxlbmd0aCA+IDApIGgucmVtb3ZlQXR0cmlidXRlKGFbMF0ubmFtZSk7CiAgfQp9KTsK"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<link rel="stylesheet" href="data:text/css,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">intro</h1>
<h4 class="author">sa21204151</h4>
<h4 class="date">2021/12/16</h4>



<div id="overview" class="section level2">
<h2>Overview</h2>
<p><em>StatComp21088</em> is a simple R package developed to achieve three kinds of gradient descent algorithm.Include Batch Gradient Descent,Stochastic Gradient Descent,Mini-batch gradient descent algorithm.</p>
</div>
<div id="shuffle_sequence-function" class="section level2">
<h2>Shuffle_sequence function</h2>
<p>The function can be used to shuffle the sequence of input data(matrix) whose fist column is filled of number one. And the output of the function is a vector of random permutation from 1 to n.Â n is the rowcount of imput data.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#library(StatComp21088)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>Shuffle_sequence <span class="ot">&lt;-</span> <span class="cf">function</span>(data){</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  random_sequence <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n,n)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(random_sequence)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n<span class="sc">*</span>n),n,n)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>shuffle_sequence <span class="ot">&lt;-</span> <span class="fu">Shuffle_sequence</span>(A)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(shuffle_sequence)</span></code></pre></div>
<pre><code>##  [1]  3  7  5  8  1 10  9  4  6  2</code></pre>
</div>
<div id="addo-function" class="section level2">
<h2>Addo function</h2>
<p>Before using the algorithm to train the model, the first column of data should be filled with 1.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>Addo <span class="ot">&lt;-</span> <span class="cf">function</span>(data){</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(<span class="fu">as.matrix</span>(data))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    c <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>,n)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    new_data <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">data.frame</span>(c,data))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(new_data)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>A <span class="ot">=</span> <span class="fu">matrix</span>(</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>),<span class="dv">4</span>,<span class="dv">2</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">Addo</span>(A)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(x)</span></code></pre></div>
<pre><code>##      c X1 X2
## [1,] 1  1  1
## [2,] 1  1  2
## [3,] 1  1  3
## [4,] 1  1  4</code></pre>
</div>
<div id="bgd-function" class="section level2">
<h2>BGD function</h2>
<p>Batch gradient descent algorithm.The idea is to use all samples per iteration to update the parameters.This function is developed to train the model one time in Batch gradient descent algorithm.The input parameters are input_data, real_result,learning rate alpha and iterative initial value theta. The output is theta after iteration.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>BGD <span class="ot">&lt;-</span> <span class="cf">function</span>(input_data,real_result,alpha,theta){</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  n0 <span class="ot">&lt;-</span> <span class="fu">nrow</span>(input_data)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  p0 <span class="ot">&lt;-</span> <span class="fu">ncol</span>(input_data)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  gradient_increasment <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>,<span class="at">nrow =</span> n0, <span class="at">ncol =</span> p0)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n0){</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>      g <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(input_data[i,]<span class="sc">*</span><span class="fu">as.vector</span>(real_result[i] <span class="sc">-</span> input_data[i,]<span class="sc">%*%</span>theta))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>      gradient_increasment[i,] <span class="ot">&lt;-</span> g</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    avg_g <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(gradient_increasment)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> theta <span class="sc">+</span> alpha <span class="sc">*</span> avg_g</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(theta)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="dv">10</span>,<span class="fl">0.002</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">5</span><span class="sc">+</span><span class="fu">rnorm</span>(n)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">data.frame</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),x))</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">BGD</span>(z, y,<span class="fl">0.002</span>,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(theta)</span></code></pre></div>
<pre><code>## [1] 1.018084 1.107645</code></pre>
</div>
<div id="sgd-function" class="section level2">
<h2>SGD function</h2>
<p>Stochastic gradient descent algorithm.The idea is to use every sample per iteration to update the parameters.This function is developed to train the model in Batch gradient descent algorithm for one iteration.The input parameters are input_data, real_result,learning rate alpha and iterative initial value theta. The output is theta after iterations.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>SGD <span class="ot">&lt;-</span> <span class="cf">function</span>(input_data,real_result,alpha,theta){</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  shuffle_sequence <span class="ot">&lt;-</span> <span class="fu">Shuffle_sequence</span>(input_data)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> input_data[shuffle_sequence,]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> real_result[shuffle_sequence]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  n0 <span class="ot">&lt;-</span> <span class="fu">nrow</span>(x)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n0){</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    g <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(x[i,]<span class="sc">*</span><span class="fu">as.vector</span>(y[i] <span class="sc">-</span> x[i,]<span class="sc">%*%</span>theta))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> theta <span class="sc">+</span> alpha <span class="sc">*</span> g</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(theta)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="dv">10</span>,<span class="fl">0.002</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">5</span><span class="sc">+</span><span class="fu">rnorm</span>(n)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">data.frame</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),x))</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">SGD</span>(z, y,<span class="fl">0.002</span>,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(theta)</span></code></pre></div>
<pre><code>## [1] 4.611148 2.053695</code></pre>
</div>
<div id="mbgd-function" class="section level2">
<h2>MBGD function</h2>
<p>This function is developed to train the model one time in Mini-batch gradient descent algorithm.The input parameters are input_data, real_result, batch_size, learning rate alpha and iterative initial value theta. The output is theta after iterations.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>MBGD <span class="ot">&lt;-</span> <span class="cf">function</span>(input_data,real_result,batch_size,alpha,theta){</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  shuffle_sequence <span class="ot">&lt;-</span> <span class="fu">Shuffle_sequence</span>(input_data)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> input_data[shuffle_sequence,]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> real_result[shuffle_sequence]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(x)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(start <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>,n,batch_size)){</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    end <span class="ot">&lt;-</span> <span class="fu">min</span>(<span class="fu">c</span>(start<span class="sc">+</span>batch_size<span class="dv">-1</span>,n))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    Mini_train_data <span class="ot">&lt;-</span> x[start<span class="sc">:</span>end,]</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    Mini_train_result <span class="ot">&lt;-</span> y[start<span class="sc">:</span>end]</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    n0 <span class="ot">&lt;-</span> <span class="fu">nrow</span>(Mini_train_data)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    p0 <span class="ot">&lt;-</span> <span class="fu">ncol</span>(Mini_train_data)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    gradient_increasment <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>,<span class="at">nrow =</span> n0, <span class="at">ncol =</span> p0)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n0){</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    g <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(Mini_train_data[i,]<span class="sc">*</span><span class="fu">as.vector</span>(Mini_train_result[i] <span class="sc">-</span> Mini_train_data[i,]<span class="sc">%*%</span>theta))</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    gradient_increasment[i,] <span class="ot">&lt;-</span> g</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    avg_g <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(gradient_increasment)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> theta <span class="sc">+</span> alpha <span class="sc">*</span> avg_g</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(theta)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="dv">10</span>,<span class="fl">0.002</span>)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">5</span><span class="sc">+</span><span class="fu">rnorm</span>(n)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">data.frame</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),x))</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">MBGD</span>(z, y, <span class="dv">100</span>,<span class="fl">0.002</span>,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(theta)</span></code></pre></div>
<pre><code>## [1] 1.316673 2.512343</code></pre>
</div>
<div id="cost-function" class="section level2">
<h2>Cost function</h2>
<p>This function is developed to record the loss function of the model. The output is the loss function value of a particular theta.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>Cost <span class="ot">&lt;-</span> <span class="cf">function</span>(input_data,real_result,theta){</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  predict <span class="ot">&lt;-</span> input_data<span class="sc">%*%</span>theta</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  cost <span class="ot">&lt;-</span> predict <span class="sc">-</span> real_result</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  cost <span class="ot">&lt;-</span> <span class="fu">mean</span>(cost<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(cost)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="dv">10</span>,<span class="fl">0.002</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">5</span><span class="sc">+</span><span class="fu">rnorm</span>(n)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">data.frame</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),x))</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>cost <span class="ot">&lt;-</span> <span class="fu">Cost</span>(z, y, <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cost)</span></code></pre></div>
<pre><code>## [1] 91.03556</code></pre>
</div>
<div id="train_bgd-function" class="section level2">
<h2>train_BGD function</h2>
<p>This function is developed to train the model for iterations in batch gradient descent algorithm and record the loss function value of the model after every iteration.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>train_BGD <span class="ot">&lt;-</span> <span class="cf">function</span>(input_data,real_result,iter,alpha,theta){</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  input_data <span class="ot">&lt;-</span> <span class="fu">Addo</span>(input_data)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  cost <span class="ot">&lt;-</span> <span class="fu">numeric</span>(iter)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iter){</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> <span class="fu">BGD</span>(input_data,real_result,alpha,theta)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    cost[i] <span class="ot">&lt;-</span> <span class="fu">Cost</span>(input_data,real_result,theta)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">theta =</span> theta ,<span class="at">cost =</span> cost))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">5</span><span class="sc">+</span>z</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">train_BGD</span>(x,y,<span class="dv">500</span>,<span class="fl">0.02</span>,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(theta)</span></code></pre></div>
<pre><code>## $theta
## [1] 4.583057 2.059439
## 
## $cost
##   [1] 12.642797  5.074068  4.314180  4.213203  4.176226  4.145697  4.116048
##   [8]  4.086737  4.057710  4.028958  4.000479  3.972270  3.944328  3.916651
##  [15]  3.889237  3.862082  3.835185  3.808543  3.782154  3.756014  3.730123
##  [22]  3.704477  3.679074  3.653911  3.628988  3.604300  3.579847  3.555626
##  [29]  3.531634  3.507869  3.484330  3.461014  3.437920  3.415044  3.392384
##  [36]  3.369940  3.347709  3.325688  3.303876  3.282271  3.260870  3.239673
##  [43]  3.218676  3.197879  3.177278  3.156873  3.136662  3.116642  3.096812
##  [50]  3.077169  3.057713  3.038442  3.019353  3.000445  2.981717  2.963165
##  [57]  2.944790  2.926589  2.908561  2.890703  2.873015  2.855494  2.838140
##  [64]  2.820950  2.803923  2.787057  2.770352  2.753804  2.737414  2.721179
##  [71]  2.705098  2.689169  2.673392  2.657763  2.642283  2.626950  2.611762
##  [78]  2.596719  2.581817  2.567057  2.552437  2.537956  2.523612  2.509404
##  [85]  2.495330  2.481390  2.467582  2.453905  2.440358  2.426939  2.413647
##  [92]  2.400482  2.387441  2.374524  2.361729  2.349055  2.336502  2.324068
##  [99]  2.311751  2.299552  2.287468  2.275498  2.263642  2.251898  2.240266
## [106]  2.228744  2.217331  2.206027  2.194830  2.183738  2.172752  2.161870
## [113]  2.151091  2.140415  2.129839  2.119364  2.108989  2.098711  2.088531
## [120]  2.078448  2.068460  2.058566  2.048767  2.039060  2.029446  2.019923
## [127]  2.010489  2.001146  1.991891  1.982723  1.973643  1.964649  1.955739
## [134]  1.946915  1.938174  1.929516  1.920940  1.912445  1.904031  1.895696
## [141]  1.887441  1.879264  1.871164  1.863141  1.855195  1.847323  1.839526
## [148]  1.831804  1.824154  1.816577  1.809071  1.801637  1.794273  1.786980
## [155]  1.779755  1.772599  1.765510  1.758489  1.751534  1.744646  1.737822
## [162]  1.731063  1.724369  1.717738  1.711169  1.704663  1.698219  1.691836
## [169]  1.685513  1.679250  1.673047  1.666902  1.660815  1.654787  1.648815
## [176]  1.642900  1.637041  1.631238  1.625490  1.619796  1.614156  1.608570
## [183]  1.603037  1.597556  1.592127  1.586749  1.581423  1.576147  1.570921
## [190]  1.565744  1.560617  1.555538  1.550507  1.545524  1.540589  1.535700
## [197]  1.530857  1.526060  1.521309  1.516603  1.511942  1.507324  1.502751
## [204]  1.498221  1.493733  1.489289  1.484886  1.480525  1.476206  1.471927
## [211]  1.467689  1.463491  1.459333  1.455215  1.451135  1.447094  1.443092
## [218]  1.439127  1.435200  1.431310  1.427457  1.423641  1.419861  1.416116
## [225]  1.412407  1.408734  1.405095  1.401490  1.397920  1.394384  1.390881
## [232]  1.387411  1.383974  1.380570  1.377198  1.373858  1.370550  1.367273
## [239]  1.364027  1.360812  1.357628  1.354473  1.351349  1.348254  1.345188
## [246]  1.342152  1.339144  1.336165  1.333214  1.330291  1.327396  1.324528
## [253]  1.321687  1.318874  1.316087  1.313326  1.310592  1.307883  1.305200
## [260]  1.302543  1.299911  1.297303  1.294721  1.292163  1.289629  1.287119
## [267]  1.284633  1.282171  1.279732  1.277316  1.274923  1.272552  1.270205
## [274]  1.267879  1.265575  1.263294  1.261034  1.258795  1.256577  1.254381
## [281]  1.252205  1.250050  1.247916  1.245801  1.243707  1.241633  1.239578
## [288]  1.237543  1.235527  1.233530  1.231552  1.229593  1.227652  1.225730
## [295]  1.223826  1.221940  1.220072  1.218222  1.216389  1.214573  1.212775
## [302]  1.210994  1.209230  1.207482  1.205751  1.204036  1.202338  1.200656
## [309]  1.198990  1.197339  1.195704  1.194085  1.192481  1.190892  1.189318
## [316]  1.187760  1.186216  1.184686  1.183171  1.181671  1.180184  1.178712
## [323]  1.177254  1.175809  1.174379  1.172962  1.171558  1.170167  1.168790
## [330]  1.167426  1.166075  1.164736  1.163410  1.162097  1.160797  1.159508
## [337]  1.158232  1.156968  1.155716  1.154475  1.153247  1.152030  1.150825
## [344]  1.149631  1.148448  1.147277  1.146117  1.144967  1.143829  1.142702
## [351]  1.141585  1.140478  1.139383  1.138297  1.137222  1.136157  1.135102
## [358]  1.134058  1.133023  1.131997  1.130982  1.129976  1.128980  1.127993
## [365]  1.127016  1.126048  1.125089  1.124139  1.123198  1.122266  1.121343
## [372]  1.120428  1.119523  1.118626  1.117737  1.116857  1.115985  1.115121
## [379]  1.114266  1.113419  1.112579  1.111748  1.110925  1.110109  1.109301
## [386]  1.108501  1.107708  1.106923  1.106145  1.105375  1.104612  1.103856
## [393]  1.103108  1.102366  1.101632  1.100904  1.100184  1.099470  1.098763
## [400]  1.098062  1.097369  1.096682  1.096001  1.095327  1.094659  1.093998
## [407]  1.093343  1.092694  1.092051  1.091414  1.090783  1.090159  1.089540
## [414]  1.088927  1.088320  1.087719  1.087123  1.086533  1.085949  1.085370
## [421]  1.084797  1.084229  1.083666  1.083109  1.082557  1.082010  1.081469
## [428]  1.080932  1.080401  1.079875  1.079354  1.078837  1.078326  1.077819
## [435]  1.077317  1.076820  1.076328  1.075841  1.075357  1.074879  1.074405
## [442]  1.073936  1.073471  1.073010  1.072554  1.072102  1.071655  1.071211
## [449]  1.070772  1.070337  1.069906  1.069480  1.069057  1.068638  1.068223
## [456]  1.067813  1.067406  1.067003  1.066603  1.066208  1.065816  1.065428
## [463]  1.065044  1.064663  1.064286  1.063913  1.063543  1.063176  1.062813
## [470]  1.062454  1.062098  1.061745  1.061396  1.061050  1.060707  1.060367
## [477]  1.060031  1.059698  1.059368  1.059041  1.058717  1.058397  1.058079
## [484]  1.057764  1.057453  1.057144  1.056838  1.056535  1.056235  1.055938
## [491]  1.055644  1.055352  1.055063  1.054777  1.054494  1.054213  1.053935
## [498]  1.053660  1.053387  1.053117</code></pre>
</div>
<div id="train_sgd-function" class="section level2">
<h2>train_SGD function</h2>
<p>This function is developed to train the model for iterations in stochastic gradient descent algorithm and record the loss function value of the model after every iteration.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>train_SGD <span class="ot">&lt;-</span> <span class="cf">function</span>(input_data,real_result,iter,alpha,theta){</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  input_data <span class="ot">&lt;-</span> <span class="fu">Addo</span>(input_data)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  cost <span class="ot">&lt;-</span> <span class="fu">numeric</span>(iter)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iter){</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> <span class="fu">SGD</span>(input_data,real_result,alpha,theta)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    cost[i] <span class="ot">&lt;-</span> <span class="fu">Cost</span>(input_data,real_result,theta)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">theta =</span> theta ,<span class="at">cost =</span> cost))</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">5</span><span class="sc">+</span>z</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">train_SGD</span>(x,y,<span class="dv">200</span>,<span class="fl">0.02</span>,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(theta)</span></code></pre></div>
<pre><code>## $theta
## [1] 4.750517 2.243376
## 
## $cost
##   [1] 1.0227839 1.5502564 1.0562684 1.1638294 1.3649958 1.3157131 1.2276249
##   [8] 1.2663037 1.0921149 1.3816171 1.1524195 1.0918089 6.9381409 1.0438423
##  [15] 1.0315171 0.9984830 3.5458747 1.0003925 1.0096645 1.4364871 0.9943503
##  [22] 0.9998517 1.7430639 3.0862112 1.0294283 1.0827036 0.9940795 1.2066007
##  [29] 0.9997756 1.1681434 1.2616260 2.4137723 1.2062579 1.0730522 1.7447972
##  [36] 1.0470192 2.2741194 1.2592948 2.8826730 1.0006938 1.0249702 1.4083589
##  [43] 5.6096110 1.2666758 1.7201295 0.9921069 1.0431104 1.0059267 2.1684693
##  [50] 1.8320542 1.2536653 2.1120384 1.7771290 1.7710228 1.1284711 3.1767351
##  [57] 1.2040697 1.3409357 0.9987223 1.8468795 1.0290723 2.0518770 1.4229813
##  [64] 3.9550777 1.5805938 1.4371097 4.2474975 1.1299751 1.0390617 1.0776667
##  [71] 6.6273134 0.9958467 1.0797509 1.6124927 1.8444789 1.0036182 3.3051331
##  [78] 1.0754776 2.2263768 1.0600681 1.2704244 2.6865672 1.0429087 1.3545052
##  [85] 1.2703922 1.4832724 1.0424688 5.4001227 3.7125455 2.0548777 1.0588210
##  [92] 1.8484427 5.5069752 1.4658872 1.3904498 1.3832826 1.1530166 1.0814401
##  [99] 1.0908378 2.0907631 1.4160551 1.2813791 6.7262167 1.2806156 1.2574954
## [106] 1.1381040 1.7472151 1.0001839 1.5908796 2.6308279 6.1682050 1.3783432
## [113] 1.9811044 1.7004438 0.9910671 1.0125461 1.1374889 1.0026003 1.0654200
## [120] 1.9464517 3.6094038 1.1028914 1.2959568 0.9963831 1.2388957 1.3003593
## [127] 1.3611447 1.1212789 1.0375728 2.5169990 1.9802604 1.0348754 2.6211429
## [134] 1.1003289 0.9940512 3.6104600 1.3985469 0.9894502 1.4924542 1.0119206
## [141] 1.5635642 1.3738364 2.1048877 6.0366083 1.0237845 2.7068923 1.0166819
## [148] 1.3507716 1.1540873 1.0086253 2.7679474 3.8511226 4.9691723 1.4165488
## [155] 1.2543832 1.5877448 2.4231878 0.9903348 1.1137192 1.0147655 1.7672925
## [162] 1.0809957 1.4064802 1.5281895 1.2529723 1.3211140 1.3006542 2.2005521
## [169] 1.0011515 1.0006843 1.2974619 0.9898476 2.1788891 0.9918600 1.4554778
## [176] 2.7860130 1.2412966 1.3670575 1.3067091 2.0471996 1.0236832 1.3369440
## [183] 1.7476774 2.0096036 5.4450537 1.7253335 1.1241673 2.5853508 1.2405574
## [190] 1.0104594 1.8434415 3.7767777 1.3267790 1.0185756 1.0418770 1.2880651
## [197] 1.0409247 1.1079439 1.0766039 2.4411122</code></pre>
</div>
<div id="train_mbgd-function" class="section level2">
<h2>train_MBGD function</h2>
<p>This function is developed to train the model in SGD algorithm for iterations in Mini-batch gradient descent algorithm and record the loss function value of the model after every iteration.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>train_MBGD <span class="ot">&lt;-</span> <span class="cf">function</span>(input_data,real_result,iter,batch_size,alpha,theta){</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  input_data <span class="ot">&lt;-</span> <span class="fu">Addo</span>(input_data)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  cost <span class="ot">&lt;-</span> <span class="fu">numeric</span>(iter)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iter){</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> <span class="fu">MBGD</span>(input_data,real_result,batch_size,alpha,theta)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    cost[i] <span class="ot">&lt;-</span> <span class="fu">Cost</span>(input_data,real_result,theta)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">theta =</span> theta ,<span class="at">cost =</span> cost))</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">5</span><span class="sc">+</span>z</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">train_MBGD</span>(x,y,<span class="dv">500</span>,<span class="dv">200</span>,<span class="fl">0.01</span>,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(theta)</span></code></pre></div>
<pre><code>## $theta
## [1] 4.955148 2.004405
## 
## $cost
##   [1] 5.518778 4.256401 4.165161 4.083734 4.011029 3.940451 3.872165 3.804608
##   [9] 3.738554 3.676245 3.611942 3.555425 3.490822 3.432448 3.376107 3.319703
##  [17] 3.265746 3.212332 3.160562 3.110825 3.061665 3.012684 2.966459 2.919389
##  [25] 2.874743 2.830501 2.787927 2.746073 2.708026 2.666520 2.626609 2.588365
##  [33] 2.551186 2.514959 2.481531 2.447444 2.411204 2.379336 2.346481 2.314662
##  [41] 2.284253 2.253926 2.225222 2.196173 2.168622 2.140992 2.114518 2.089259
##  [49] 2.062996 2.038228 2.014788 1.991162 1.968103 1.945631 1.923687 1.901368
##  [57] 1.880414 1.859922 1.839937 1.821539 1.801334 1.782751 1.764776 1.746969
##  [65] 1.729482 1.712431 1.695865 1.679742 1.664015 1.648783 1.633741 1.618925
##  [73] 1.604522 1.590523 1.576982 1.563662 1.550500 1.538139 1.525378 1.513176
##  [81] 1.502160 1.489837 1.478528 1.467483 1.456706 1.446750 1.435840 1.426027
##  [89] 1.416667 1.407091 1.397458 1.388168 1.379058 1.370486 1.362029 1.353594
##  [97] 1.345503 1.337573 1.331967 1.322738 1.315146 1.308408 1.300701 1.294423
## [105] 1.287228 1.280879 1.274237 1.268033 1.262007 1.258820 1.250194 1.245251
## [113] 1.238964 1.233612 1.228824 1.223708 1.218160 1.213163 1.208649 1.203941
## [121] 1.199345 1.194693 1.190308 1.186255 1.182157 1.178101 1.174661 1.169971
## [129] 1.167039 1.162782 1.159795 1.155382 1.153519 1.149170 1.145848 1.142340
## [137] 1.139348 1.136151 1.132947 1.130064 1.127367 1.124400 1.121827 1.119037
## [145] 1.116884 1.114710 1.111614 1.109039 1.106750 1.104599 1.103037 1.100000
## [153] 1.097827 1.095715 1.093725 1.093248 1.090397 1.088687 1.085995 1.084195
## [161] 1.083117 1.081015 1.079031 1.077417 1.076061 1.074389 1.073411 1.071178
## [169] 1.069960 1.068384 1.067221 1.065737 1.064331 1.063287 1.061700 1.060407
## [177] 1.059187 1.058273 1.056880 1.055867 1.055192 1.053612 1.052598 1.051493
## [185] 1.051189 1.050803 1.049002 1.047631 1.046735 1.046412 1.045002 1.045097
## [193] 1.043303 1.042548 1.042110 1.040952 1.040515 1.039486 1.039221 1.038103
## [201] 1.038418 1.037145 1.036115 1.036613 1.034850 1.034575 1.033785 1.033985
## [209] 1.034121 1.032003 1.031849 1.030835 1.031232 1.029822 1.029955 1.028941
## [217] 1.028424 1.028038 1.027490 1.027167 1.026839 1.026362 1.026681 1.025938
## [225] 1.025451 1.024724 1.024470 1.024396 1.023680 1.023812 1.022913 1.022679
## [233] 1.022460 1.022119 1.021653 1.021766 1.021434 1.020811 1.020738 1.020497
## [241] 1.020019 1.020165 1.019591 1.019223 1.019395 1.019191 1.019574 1.018307
## [249] 1.018081 1.018144 1.017654 1.017677 1.017271 1.017482 1.016871 1.017327
## [257] 1.016503 1.016748 1.016173 1.016115 1.015818 1.015711 1.015513 1.016051
## [265] 1.015353 1.015368 1.014953 1.014960 1.014681 1.014507 1.014567 1.014332
## [273] 1.014923 1.014010 1.013907 1.013966 1.013692 1.014467 1.013668 1.013540
## [281] 1.013308 1.013202 1.013441 1.013039 1.012899 1.012768 1.012718 1.012597
## [289] 1.012542 1.012464 1.012705 1.012273 1.012253 1.012217 1.012777 1.012341
## [297] 1.011960 1.012117 1.011832 1.011799 1.011668 1.011632 1.011654 1.011490
## [305] 1.011598 1.012313 1.011327 1.011341 1.011525 1.011139 1.011235 1.011390
## [313] 1.011000 1.011009 1.010942 1.010873 1.010816 1.011001 1.010944 1.011016
## [321] 1.010648 1.010767 1.010591 1.010658 1.010569 1.010476 1.010735 1.010414
## [329] 1.010392 1.010726 1.010506 1.010870 1.011171 1.010291 1.010729 1.010260
## [337] 1.010316 1.010132 1.010166 1.010125 1.010175 1.010322 1.010316 1.009958
## [345] 1.010730 1.010143 1.009906 1.009928 1.009870 1.009921 1.009891 1.009794
## [353] 1.011243 1.009762 1.009744 1.009889 1.011704 1.010167 1.009682 1.009664
## [361] 1.009705 1.009647 1.009937 1.009873 1.009951 1.009580 1.009561 1.009609
## [369] 1.009536 1.009524 1.009927 1.009500 1.009497 1.010244 1.010578 1.009520
## [377] 1.009450 1.009585 1.009640 1.009507 1.009476 1.009489 1.009469 1.009498
## [385] 1.009411 1.009535 1.009355 1.009395 1.009537 1.009341 1.012516 1.009434
## [393] 1.009460 1.009316 1.010735 1.009309 1.009548 1.009277 1.009779 1.009304
## [401] 1.009679 1.009546 1.009395 1.009530 1.009654 1.009282 1.009262 1.009293
## [409] 1.009225 1.009225 1.009230 1.009538 1.009200 1.009250 1.009236 1.009198
## [417] 1.009308 1.009861 1.009843 1.009859 1.009250 1.009218 1.009385 1.009858
## [425] 1.009334 1.010100 1.009292 1.009161 1.009157 1.009140 1.009143 1.009177
## [433] 1.009380 1.009301 1.009138 1.009123 1.010506 1.009197 1.009168 1.009157
## [441] 1.009337 1.009138 1.009399 1.009165 1.009256 1.009566 1.009197 1.009128
## [449] 1.009190 1.009637 1.009126 1.009740 1.009271 1.009086 1.009491 1.009355
## [457] 1.009213 1.009199 1.009106 1.009324 1.009091 1.009847 1.009226 1.009080
## [465] 1.009071 1.009629 1.009143 1.009072 1.009162 1.009615 1.009447 1.009066
## [473] 1.009080 1.009333 1.009060 1.009143 1.009385 1.009841 1.009309 1.009192
## [481] 1.009220 1.009186 1.009251 1.009065 1.009634 1.009151 1.009134 1.009371
## [489] 1.009077 1.009150 1.010273 1.009360 1.009441 1.009197 1.009734 1.009756
## [497] 1.009046 1.009114 1.009860 1.009041</code></pre>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
